{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd895d2",
   "metadata": {},
   "source": [
    "# Breast Cancer Prediction - Hyperparameter Tuning\n",
    "This notebook tunes Decision Tree, K-Nearest Neighbors, and Logistic Regression using GridSearchCV and evaluates their optimized performance on the test set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbdaf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import appropriate libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5089df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Number of samples: 569\n",
      "Number of features: 30\n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset\n",
    "try:\n",
    "    cancer = load_breast_cancer()\n",
    "\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(f\"Number of samples: {cancer.data.shape[0]}\")\n",
    "    print(f\"Number of features: {cancer.data.shape[1]}\")\n",
    "\n",
    "except:\n",
    "    print(\"Error loading the dataset. Please ensure you have the required libraries installed.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46409789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset for model training, testing and validation\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a08c1bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features kept after variance threshold filtering:\n",
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area' 'radius error'\n",
      " 'texture error' 'perimeter error' 'area error' 'worst radius'\n",
      " 'worst texture' 'worst perimeter' 'worst area' 'worst compactness'\n",
      " 'worst concavity']\n"
     ]
    }
   ],
   "source": [
    "# Variance Threshold Filtering\n",
    "selector = VarianceThreshold(threshold=0.01)\n",
    "X_train_var = selector.fit_transform(X_train)\n",
    "var_mask = selector.get_support()\n",
    "selected_features = np.array(cancer.feature_names)[var_mask]\n",
    "print(\"Features kept after variance threshold filtering:\")\n",
    "print(selected_features)\n",
    "\n",
    "X_val_var = selector.transform(X_val)\n",
    "X_test_var = selector.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9fc5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by RFE:\n",
      "['mean perimeter' 'mean area' 'radius error' 'texture error' 'area error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst concavity']\n",
      "Ranking of features by RFE:\n",
      "[3 5 1 1 1 1 4 1 1 1 1 1 2 1]\n",
      "RFE score:\n",
      "0.9648093841642229\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_var)\n",
    "\n",
    "# Feature Selection using RFE\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "rfe = RFE(lr, n_features_to_select=10)\n",
    "X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Features selected by RFE:\")\n",
    "print(np.array(cancer.feature_names)[var_mask][rfe.get_support()])\n",
    "print(\"Ranking of features by RFE:\")\n",
    "print(rfe.ranking_)\n",
    "print(\"RFE score:\")\n",
    "print(rfe.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b846a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by RFE:\n",
      "['area error' 'worst radius' 'worst texture' 'worst area'\n",
      " 'worst concavity']\n",
      "Ranking of features by RFE:\n",
      "[ 8 10  5  6  3  4  9  1  1  1  2  1  7  1]\n",
      "RFE score:\n",
      "0.9648093841642229\n"
     ]
    }
   ],
   "source": [
    "rfe = RFE(lr, n_features_to_select=5)\n",
    "X_train_rfe = rfe.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Features selected by RFE:\")\n",
    "print(np.array(cancer.feature_names)[var_mask][rfe.get_support()])\n",
    "print(\"Ranking of features by RFE:\")\n",
    "print(rfe.ranking_)\n",
    "print(\"RFE score:\")\n",
    "print(rfe.score(X_train_scaled, y_train))\n",
    "\n",
    "# Save the mask for later use on val/test\n",
    "rfe_mask = rfe.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65deb1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best params: {'logreg__C': 0.1, 'logreg__penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('logreg', LogisticRegression(max_iter=1000, solver='liblinear'))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'logreg__penalty': ['l1', 'l2'],\n",
    "    'logreg__C': [0.01, 0.1, 1, 10, 100],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "grid.fit(X_train_var[:, rfe_mask], y_train)\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b512b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92        44\n",
      "           1       0.97      0.93      0.95        70\n",
      "\n",
      "    accuracy                           0.94       114\n",
      "   macro avg       0.93      0.94      0.94       114\n",
      "weighted avg       0.94      0.94      0.94       114\n",
      "\n",
      "Confusion Matrix:\n",
      " [[42  2]\n",
      " [ 5 65]]\n",
      "MSE: 0.06140350877192982\n",
      "Test Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.95      0.95        43\n",
      "           1       0.97      0.97      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.96      0.96       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "Confusion Matrix:\n",
      " [[41  2]\n",
      " [ 2 69]]\n",
      "MSE: 0.03508771929824561\n"
     ]
    }
   ],
   "source": [
    "# Validation performance\n",
    "y_val_pred = grid.predict(X_val_var[:, rfe_mask])\n",
    "print(\"Validation Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
    "print(\"MSE:\", np.mean((y_val - y_val_pred) ** 2))\n",
    "\n",
    "# Final test performance\n",
    "y_test_pred = grid.predict(X_test_var[:, rfe_mask])\n",
    "print(\"Test Report:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "print(\"MSE:\", np.mean((y_test - y_test_pred) ** 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
